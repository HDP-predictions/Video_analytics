{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 1\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "#tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "#session_conf = tf.ConfigProto()\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)\n",
    "# for later versions:\n",
    "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import LSTM\n",
    "#import numpy as np\n",
    "import glob\n",
    "from imageio import imread\n",
    "from skimage.transform import resize as imresize\n",
    "from keras.layers import LeakyReLU\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "seq = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2053) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2,2054,2060,2061,2065,2066,2067,2068) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_zero_padded.csv')\n",
    "val = pd.read_csv('validation_zero_padded_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.drop('labels',axis=1,inplace=True)\n",
    "#val.drop('t',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "train.drop('tags',axis=1,inplace=True)\n",
    "val.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#val.drop('Unnamed: 0.1',axis=1,inplace=True)\n",
    "val.drop('tags',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0.1'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-6af7d461f70a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unnamed: 0.1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3997\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3998\u001b[0m         )\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3934\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3936\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3970\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5017\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5018\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5020\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Unnamed: 0.1'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#train.drop('Unnamed: 0.1',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('labels',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = set(train.columns)\n",
    "s2 = set(val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 - s2.intersection(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.drop(['From (min.sec)',\n",
    " 'Label',\n",
    " 'Procedure or manipulation (Name)',\n",
    " 'To (min.sec)',\n",
    " 'Total Duration of video file (min.sec)',\n",
    " 'UHID',\n",
    " 'Video (Title)',\n",
    " 't',\n",
    " 'time','title_y'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.rename(columns={'title_x':'title'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df.uhid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(LSTM(256,dropout=0.2,input_shape=(None,2048)))\n",
    "model.add(LSTM(512,return_sequences=True,activation='tanh',input_shape=(None,2048)))\n",
    "#model.add(activation='tanh')\n",
    "model.add(LSTM(256,return_sequences=True))\n",
    "model.add(LSTM(128,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "es =   EarlyStopping(monitor='val_loss', patience=8, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ids:\n",
    "    patient = df[df['uhid']==i]\n",
    "    total_frames = patient['Total_frames'].unique()\n",
    "    for j in total_frames:\n",
    "        x = patient[patient['Total_frames']==j]\n",
    "        x = x.sort_values(by=['series','frame_number'])             \n",
    "        x['diaper_change'].fillna(method='ffill',inplace=True)\n",
    "        x['feeding'].fillna(method='ffill',inplace=True)\n",
    "        x['patting'].fillna(method='ffill',inplace=True)\n",
    "        X = X.append(x,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop('title',axis=1,inplace=True)\n",
    "#X.drop('uhid',axis=1,inplace=True)\n",
    "X.drop('Total_frames',axis=1,inplace=True)\n",
    "X.drop('frame_number',axis=1,inplace=True)\n",
    "X.drop('series',axis=1,inplace=True)\n",
    "#X.drop('labels',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.drop('labels',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.drop('tags',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)/120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df.uhid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200101135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 309 samples, validate on 78 samples\n",
      "Epoch 1/40\n",
      "309/309 [==============================] - 6s 19ms/step - loss: 1.1052 - accuracy: 0.4498 - val_loss: 0.8888 - val_accuracy: 0.5385\n",
      "Epoch 2/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 0.7326 - accuracy: 0.7540 - val_loss: 0.4045 - val_accuracy: 0.8718\n",
      "Epoch 3/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 0.4148 - accuracy: 0.8964 - val_loss: 0.2153 - val_accuracy: 0.9487\n",
      "Epoch 4/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 0.3048 - accuracy: 0.9191 - val_loss: 0.2997 - val_accuracy: 0.9231\n",
      "Epoch 5/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 0.2226 - accuracy: 0.9353 - val_loss: 0.3497 - val_accuracy: 0.9103\n",
      "Epoch 6/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 0.1664 - accuracy: 0.9385 - val_loss: 0.3799 - val_accuracy: 0.8974\n",
      "Epoch 7/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 0.1227 - accuracy: 0.9579 - val_loss: 0.4774 - val_accuracy: 0.8846\n",
      "Epoch 8/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 0.0941 - accuracy: 0.9709 - val_loss: 0.5799 - val_accuracy: 0.8205\n",
      "Epoch 9/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 0.0729 - accuracy: 0.9806 - val_loss: 0.6374 - val_accuracy: 0.8077\n",
      "Epoch 10/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 0.0484 - accuracy: 0.9871 - val_loss: 0.7455 - val_accuracy: 0.8205\n",
      "Epoch 11/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 0.0376 - accuracy: 0.9935 - val_loss: 0.7554 - val_accuracy: 0.8333\n",
      "191001977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180 samples, validate on 46 samples\n",
      "Epoch 1/40\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.2564 - val_accuracy: 0.6739\n",
      "Epoch 2/40\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2361 - val_accuracy: 0.6522\n",
      "Epoch 3/40\n",
      "180/180 [==============================] - 3s 18ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2628 - val_accuracy: 0.6522\n",
      "Epoch 4/40\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3208 - val_accuracy: 0.6522\n",
      "Epoch 5/40\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.3870 - val_accuracy: 0.6522\n",
      "Epoch 6/40\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.4488 - val_accuracy: 0.6522\n",
      "Epoch 7/40\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4994 - val_accuracy: 0.6522\n",
      "Epoch 8/40\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.5396 - val_accuracy: 0.6522\n",
      "Epoch 9/40\n",
      "180/180 [==============================] - 3s 18ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.5714 - val_accuracy: 0.6522\n",
      "Epoch 10/40\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 9.1030e-04 - accuracy: 1.0000 - val_loss: 1.5966 - val_accuracy: 0.6522\n",
      "200100035\n",
      "Train on 257 samples, validate on 65 samples\n",
      "Epoch 1/40\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.2045 - accuracy: 0.9533 - val_loss: 1.3006 - val_accuracy: 0.7538\n",
      "Epoch 2/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0854 - accuracy: 0.9611 - val_loss: 0.9289 - val_accuracy: 0.8000\n",
      "Epoch 3/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.1127 - accuracy: 0.9455 - val_loss: 0.9917 - val_accuracy: 0.7692\n",
      "Epoch 4/40\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.1198 - accuracy: 0.9689 - val_loss: 0.8729 - val_accuracy: 0.8000\n",
      "Epoch 5/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0524 - accuracy: 0.9883 - val_loss: 0.6981 - val_accuracy: 0.8615\n",
      "Epoch 6/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0341 - accuracy: 0.9922 - val_loss: 0.6478 - val_accuracy: 0.8769\n",
      "Epoch 7/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0300 - accuracy: 0.9883 - val_loss: 0.7057 - val_accuracy: 0.8615\n",
      "Epoch 8/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.7773 - val_accuracy: 0.8615\n",
      "Epoch 9/40\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.7577 - val_accuracy: 0.8615\n",
      "Epoch 10/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.7266 - val_accuracy: 0.8923\n",
      "Epoch 11/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7190 - val_accuracy: 0.8923\n",
      "Epoch 12/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7252 - val_accuracy: 0.8923\n",
      "Epoch 13/40\n",
      "257/257 [==============================] - 4s 16ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.7351 - val_accuracy: 0.8923\n",
      "Epoch 14/40\n",
      "257/257 [==============================] - 4s 17ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7462 - val_accuracy: 0.8923\n",
      "200100035\n",
      "Train on 312 samples, validate on 79 samples\n",
      "Epoch 1/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0176 - accuracy: 0.9968 - val_loss: 0.3280 - val_accuracy: 0.9620\n",
      "Epoch 2/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0058 - accuracy: 0.9968 - val_loss: 0.4086 - val_accuracy: 0.9241\n",
      "Epoch 3/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 0.8481\n",
      "Epoch 4/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.9337 - val_accuracy: 0.7848\n",
      "Epoch 5/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.0404 - val_accuracy: 0.7848\n",
      "Epoch 6/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0333 - val_accuracy: 0.7848\n",
      "Epoch 7/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9929 - val_accuracy: 0.7975\n",
      "Epoch 8/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 8.1598e-04 - accuracy: 1.0000 - val_loss: 0.9543 - val_accuracy: 0.8101\n",
      "Epoch 9/40\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 7.4399e-04 - accuracy: 1.0000 - val_loss: 0.9067 - val_accuracy: 0.8228\n",
      "RSHI.0000023451\n",
      "Train on 309 samples, validate on 78 samples\n",
      "Epoch 1/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 6.9288e-04 - accuracy: 1.0000 - val_loss: 0.8619 - val_accuracy: 0.8333\n",
      "Epoch 2/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 6.4409e-04 - accuracy: 1.0000 - val_loss: 0.8346 - val_accuracy: 0.8333\n",
      "Epoch 3/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 5.9530e-04 - accuracy: 1.0000 - val_loss: 0.8185 - val_accuracy: 0.8333\n",
      "Epoch 4/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 5.5562e-04 - accuracy: 1.0000 - val_loss: 0.8103 - val_accuracy: 0.8333\n",
      "Epoch 5/40\n",
      "309/309 [==============================] - 5s 16ms/step - loss: 5.2447e-04 - accuracy: 1.0000 - val_loss: 0.8125 - val_accuracy: 0.8462\n",
      "Epoch 6/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 5.0068e-04 - accuracy: 1.0000 - val_loss: 0.8217 - val_accuracy: 0.8462\n",
      "Epoch 7/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 4.7804e-04 - accuracy: 1.0000 - val_loss: 0.8322 - val_accuracy: 0.8462\n",
      "Epoch 8/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 4.6109e-04 - accuracy: 1.0000 - val_loss: 0.8429 - val_accuracy: 0.8462\n",
      "Epoch 9/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 4.4470e-04 - accuracy: 1.0000 - val_loss: 0.8535 - val_accuracy: 0.8462\n",
      "Epoch 10/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 4.3048e-04 - accuracy: 1.0000 - val_loss: 0.8633 - val_accuracy: 0.8462\n",
      "Epoch 11/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 4.1812e-04 - accuracy: 1.0000 - val_loss: 0.8720 - val_accuracy: 0.8333\n",
      "Epoch 12/40\n",
      "309/309 [==============================] - 5s 17ms/step - loss: 4.0532e-04 - accuracy: 1.0000 - val_loss: 0.8784 - val_accuracy: 0.8333\n",
      "200201148\n",
      "Train on 312 samples, validate on 79 samples\n",
      "Epoch 1/40\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 0.0140 - accuracy: 0.9968 - val_loss: 1.2620 - val_accuracy: 0.7848\n",
      "Epoch 2/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.8734\n",
      "Epoch 3/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 4.6964e-04 - accuracy: 1.0000 - val_loss: 0.5758 - val_accuracy: 0.9241\n",
      "Epoch 4/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 8.8043e-04 - accuracy: 1.0000 - val_loss: 0.5769 - val_accuracy: 0.9241\n",
      "Epoch 5/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5489 - val_accuracy: 0.9367\n",
      "Epoch 6/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 5.1877e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.9367\n",
      "Epoch 7/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 5.7771e-04 - accuracy: 1.0000 - val_loss: 0.5203 - val_accuracy: 0.9367\n",
      "Epoch 8/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 6.8185e-04 - accuracy: 1.0000 - val_loss: 0.5133 - val_accuracy: 0.9367\n",
      "Epoch 9/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 7.5165e-04 - accuracy: 1.0000 - val_loss: 0.5126 - val_accuracy: 0.9367\n",
      "Epoch 10/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 6.2677e-04 - accuracy: 1.0000 - val_loss: 0.5176 - val_accuracy: 0.9367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 4.7240e-04 - accuracy: 1.0000 - val_loss: 0.5232 - val_accuracy: 0.9367\n",
      "Epoch 12/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 4.1795e-04 - accuracy: 1.0000 - val_loss: 0.5285 - val_accuracy: 0.9367\n",
      "Epoch 13/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 3.8819e-04 - accuracy: 1.0000 - val_loss: 0.5333 - val_accuracy: 0.9367\n",
      "Epoch 14/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 3.6811e-04 - accuracy: 1.0000 - val_loss: 0.5373 - val_accuracy: 0.9367\n",
      "Epoch 15/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 3.5208e-04 - accuracy: 1.0000 - val_loss: 0.5408 - val_accuracy: 0.9367\n",
      "Epoch 16/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 3.3935e-04 - accuracy: 1.0000 - val_loss: 0.5439 - val_accuracy: 0.9367\n",
      "Epoch 17/40\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 3.2838e-04 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.9367\n",
      "200201148\n",
      "Train on 301 samples, validate on 76 samples\n",
      "Epoch 1/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 3.5763e-04 - accuracy: 1.0000 - val_loss: 0.6472 - val_accuracy: 0.9211\n",
      "Epoch 2/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 3.4400e-04 - accuracy: 1.0000 - val_loss: 0.6496 - val_accuracy: 0.9211\n",
      "Epoch 3/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 3.2748e-04 - accuracy: 1.0000 - val_loss: 0.6516 - val_accuracy: 0.9211\n",
      "Epoch 4/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 3.1541e-04 - accuracy: 1.0000 - val_loss: 0.6536 - val_accuracy: 0.9211\n",
      "Epoch 5/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 3.0342e-04 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.9211\n",
      "Epoch 6/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 2.9342e-04 - accuracy: 1.0000 - val_loss: 0.6577 - val_accuracy: 0.9211\n",
      "Epoch 7/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 2.8377e-04 - accuracy: 1.0000 - val_loss: 0.6598 - val_accuracy: 0.9211\n",
      "Epoch 8/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 2.7593e-04 - accuracy: 1.0000 - val_loss: 0.6619 - val_accuracy: 0.9211\n",
      "Epoch 9/40\n",
      "301/301 [==============================] - 5s 16ms/step - loss: 2.6744e-04 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 0.9211\n",
      "200100147\n",
      "Train on 304 samples, validate on 77 samples\n",
      "Epoch 1/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 2.4583e-04 - accuracy: 1.0000 - val_loss: 0.5381 - val_accuracy: 0.9221\n",
      "Epoch 2/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 2.3997e-04 - accuracy: 1.0000 - val_loss: 0.5416 - val_accuracy: 0.9221\n",
      "Epoch 3/40\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 2.3486e-04 - accuracy: 1.0000 - val_loss: 0.5447 - val_accuracy: 0.9091\n",
      "Epoch 4/40\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 2.3004e-04 - accuracy: 1.0000 - val_loss: 0.5477 - val_accuracy: 0.9091\n",
      "Epoch 5/40\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 2.2520e-04 - accuracy: 1.0000 - val_loss: 0.5505 - val_accuracy: 0.9091\n",
      "Epoch 6/40\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 2.2097e-04 - accuracy: 1.0000 - val_loss: 0.5530 - val_accuracy: 0.9091\n",
      "Epoch 7/40\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 2.1673e-04 - accuracy: 1.0000 - val_loss: 0.5553 - val_accuracy: 0.9091\n",
      "Epoch 8/40\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 2.1275e-04 - accuracy: 1.0000 - val_loss: 0.5577 - val_accuracy: 0.9091\n",
      "Epoch 9/40\n",
      "304/304 [==============================] - 5s 17ms/step - loss: 2.0879e-04 - accuracy: 1.0000 - val_loss: 0.5599 - val_accuracy: 0.9091\n",
      "200100263\n",
      "Train on 310 samples, validate on 78 samples\n",
      "Epoch 1/40\n",
      "310/310 [==============================] - 5s 17ms/step - loss: 2.0322e-04 - accuracy: 1.0000 - val_loss: 0.5549 - val_accuracy: 0.9103\n",
      "Epoch 2/40\n",
      "310/310 [==============================] - 5s 16ms/step - loss: 1.9969e-04 - accuracy: 1.0000 - val_loss: 0.5570 - val_accuracy: 0.9103\n",
      "Epoch 3/40\n",
      "310/310 [==============================] - 5s 16ms/step - loss: 1.9640e-04 - accuracy: 1.0000 - val_loss: 0.5590 - val_accuracy: 0.9103\n",
      "Epoch 4/40\n",
      "310/310 [==============================] - 5s 17ms/step - loss: 1.9312e-04 - accuracy: 1.0000 - val_loss: 0.5609 - val_accuracy: 0.9103\n",
      "Epoch 5/40\n",
      "310/310 [==============================] - 5s 17ms/step - loss: 1.8993e-04 - accuracy: 1.0000 - val_loss: 0.5628 - val_accuracy: 0.9103\n",
      "Epoch 6/40\n",
      "310/310 [==============================] - 5s 17ms/step - loss: 1.8692e-04 - accuracy: 1.0000 - val_loss: 0.5647 - val_accuracy: 0.9103\n",
      "Epoch 7/40\n",
      "310/310 [==============================] - 5s 17ms/step - loss: 1.8399e-04 - accuracy: 1.0000 - val_loss: 0.5666 - val_accuracy: 0.9103\n",
      "Epoch 8/40\n",
      "310/310 [==============================] - 5s 17ms/step - loss: 1.8120e-04 - accuracy: 1.0000 - val_loss: 0.5683 - val_accuracy: 0.9103\n",
      "Epoch 9/40\n",
      "310/310 [==============================] - 5s 17ms/step - loss: 1.7847e-04 - accuracy: 1.0000 - val_loss: 0.5700 - val_accuracy: 0.9103\n",
      "200300181\n",
      "Train on 304 samples, validate on 76 samples\n",
      "Epoch 1/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.7965e-04 - accuracy: 1.0000 - val_loss: 0.9159 - val_accuracy: 0.8684\n",
      "Epoch 2/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.7698e-04 - accuracy: 1.0000 - val_loss: 0.9185 - val_accuracy: 0.8553\n",
      "Epoch 3/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.7420e-04 - accuracy: 1.0000 - val_loss: 0.9213 - val_accuracy: 0.8553\n",
      "Epoch 4/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.7175e-04 - accuracy: 1.0000 - val_loss: 0.9240 - val_accuracy: 0.8553\n",
      "Epoch 5/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.6914e-04 - accuracy: 1.0000 - val_loss: 0.9266 - val_accuracy: 0.8553\n",
      "Epoch 6/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.6673e-04 - accuracy: 1.0000 - val_loss: 0.9292 - val_accuracy: 0.8553\n",
      "Epoch 7/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.6426e-04 - accuracy: 1.0000 - val_loss: 0.9317 - val_accuracy: 0.8553\n",
      "Epoch 8/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.6195e-04 - accuracy: 1.0000 - val_loss: 0.9342 - val_accuracy: 0.8553\n",
      "Epoch 9/40\n",
      "304/304 [==============================] - 5s 16ms/step - loss: 1.5970e-04 - accuracy: 1.0000 - val_loss: 0.9366 - val_accuracy: 0.8553\n",
      "200300181\n",
      "Train on 296 samples, validate on 74 samples\n",
      "Epoch 1/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.5644e-04 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 0.9189\n",
      "Epoch 2/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.5428e-04 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 0.9189\n",
      "Epoch 3/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.5221e-04 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 0.9189\n",
      "Epoch 4/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.5014e-04 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 0.9189\n",
      "Epoch 5/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.4828e-04 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 0.9189\n",
      "Epoch 6/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.4630e-04 - accuracy: 1.0000 - val_loss: 0.6100 - val_accuracy: 0.9189\n",
      "Epoch 7/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.4444e-04 - accuracy: 1.0000 - val_loss: 0.6112 - val_accuracy: 0.9189\n",
      "Epoch 8/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.4256e-04 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 0.9189\n",
      "Epoch 9/40\n",
      "296/296 [==============================] - 5s 16ms/step - loss: 1.4078e-04 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 0.9189\n",
      "RSHI.0000021833\n",
      "Train on 285 samples, validate on 72 samples\n",
      "Epoch 1/40\n",
      "285/285 [==============================] - 5s 17ms/step - loss: 1.4131e-04 - accuracy: 1.0000 - val_loss: 0.6479 - val_accuracy: 0.9028\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 5s 17ms/step - loss: 1.3957e-04 - accuracy: 1.0000 - val_loss: 0.6491 - val_accuracy: 0.8889\n",
      "Epoch 3/40\n",
      "285/285 [==============================] - 5s 17ms/step - loss: 1.3776e-04 - accuracy: 1.0000 - val_loss: 0.6502 - val_accuracy: 0.8889\n",
      "Epoch 4/40\n",
      "285/285 [==============================] - 5s 17ms/step - loss: 1.3614e-04 - accuracy: 1.0000 - val_loss: 0.6513 - val_accuracy: 0.8889\n",
      "Epoch 5/40\n",
      "285/285 [==============================] - 5s 17ms/step - loss: 1.3446e-04 - accuracy: 1.0000 - val_loss: 0.6525 - val_accuracy: 0.8889\n",
      "Epoch 6/40\n",
      "285/285 [==============================] - 5s 17ms/step - loss: 1.3281e-04 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 0.8889\n",
      "Epoch 7/40\n",
      "285/285 [==============================] - 5s 17ms/step - loss: 1.3117e-04 - accuracy: 1.0000 - val_loss: 0.6551 - val_accuracy: 0.8889\n",
      "Epoch 8/40\n",
      "285/285 [==============================] - 5s 17ms/step - loss: 1.2968e-04 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.8889\n",
      "Epoch 9/40\n",
      "285/285 [==============================] - 5s 17ms/step - loss: 1.2806e-04 - accuracy: 1.0000 - val_loss: 0.6577 - val_accuracy: 0.8889\n",
      "RSHI.0000024482\n",
      "Train on 280 samples, validate on 70 samples\n",
      "Epoch 1/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.2735e-04 - accuracy: 1.0000 - val_loss: 7.9187e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.2584e-04 - accuracy: 1.0000 - val_loss: 7.8353e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.2435e-04 - accuracy: 1.0000 - val_loss: 7.7500e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.2290e-04 - accuracy: 1.0000 - val_loss: 7.6686e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.2151e-04 - accuracy: 1.0000 - val_loss: 7.5865e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.2007e-04 - accuracy: 1.0000 - val_loss: 7.5048e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.1877e-04 - accuracy: 1.0000 - val_loss: 7.4244e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.1740e-04 - accuracy: 1.0000 - val_loss: 7.3452e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.1611e-04 - accuracy: 1.0000 - val_loss: 7.2681e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/40\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 1.1481e-04 - accuracy: 1.0000 - val_loss: 7.1923e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.1354e-04 - accuracy: 1.0000 - val_loss: 7.1179e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.1232e-04 - accuracy: 1.0000 - val_loss: 7.0452e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.1112e-04 - accuracy: 1.0000 - val_loss: 6.9721e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0994e-04 - accuracy: 1.0000 - val_loss: 6.9028e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0877e-04 - accuracy: 1.0000 - val_loss: 6.8337e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0761e-04 - accuracy: 1.0000 - val_loss: 6.7645e-05 - val_accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0649e-04 - accuracy: 1.0000 - val_loss: 6.6966e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0542e-04 - accuracy: 1.0000 - val_loss: 6.6310e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0433e-04 - accuracy: 1.0000 - val_loss: 6.5670e-05 - val_accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0329e-04 - accuracy: 1.0000 - val_loss: 6.5031e-05 - val_accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0227e-04 - accuracy: 1.0000 - val_loss: 6.4398e-05 - val_accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0123e-04 - accuracy: 1.0000 - val_loss: 6.3780e-05 - val_accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 1.0023e-04 - accuracy: 1.0000 - val_loss: 6.3187e-05 - val_accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.9236e-05 - accuracy: 1.0000 - val_loss: 6.2588e-05 - val_accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.8262e-05 - accuracy: 1.0000 - val_loss: 6.1990e-05 - val_accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.7298e-05 - accuracy: 1.0000 - val_loss: 6.1398e-05 - val_accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.6348e-05 - accuracy: 1.0000 - val_loss: 6.0820e-05 - val_accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.5403e-05 - accuracy: 1.0000 - val_loss: 6.0258e-05 - val_accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.4476e-05 - accuracy: 1.0000 - val_loss: 5.9713e-05 - val_accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.3576e-05 - accuracy: 1.0000 - val_loss: 5.9168e-05 - val_accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.2718e-05 - accuracy: 1.0000 - val_loss: 5.8639e-05 - val_accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.1842e-05 - accuracy: 1.0000 - val_loss: 5.8116e-05 - val_accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.0999e-05 - accuracy: 1.0000 - val_loss: 5.7598e-05 - val_accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 9.0133e-05 - accuracy: 1.0000 - val_loss: 5.7093e-05 - val_accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 8.9312e-05 - accuracy: 1.0000 - val_loss: 5.6595e-05 - val_accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 8.8504e-05 - accuracy: 1.0000 - val_loss: 5.6112e-05 - val_accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 8.7679e-05 - accuracy: 1.0000 - val_loss: 5.5628e-05 - val_accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 8.6890e-05 - accuracy: 1.0000 - val_loss: 5.5139e-05 - val_accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 8.6120e-05 - accuracy: 1.0000 - val_loss: 5.4663e-05 - val_accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 8.5357e-05 - accuracy: 1.0000 - val_loss: 5.4203e-05 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for i in ids:\n",
    "    #try:\n",
    "    print(i)\n",
    "    X_val = X[X['uhid']==i]\n",
    "    X_train = X[X['uhid']!=i]\n",
    "    X_val.drop('uhid',axis=1,inplace=True)\n",
    "    X_train.drop('uhid',axis=1,inplace=True)\n",
    "    train_labels1 = X_train[['diaper_change','feeding','patting']]\n",
    "    validation_labels1 = X_val[['diaper_change','feeding','patting']]\n",
    "    train_labels = []\n",
    "    for i in range(0,len(train_labels1),seq):\n",
    "    #print(i)\n",
    "        train_labels1 = np.array(train_labels1)\n",
    "        y1 = train_labels1[i:i+seq]\n",
    "        train_labels.append(y1[-4])\n",
    "\n",
    "    validation_labels = []\n",
    "    for i in range(0,len(validation_labels1),seq):\n",
    "    #print(i)\n",
    "        validation_labels1 = np.array(validation_labels1)\n",
    "        y2 = validation_labels1[i:i+seq]\n",
    "        validation_labels.append(y2[-4])\n",
    "    X_train.drop(['diaper_change','feeding','patting'],axis=1,inplace=True)\n",
    "    X_val.drop(['diaper_change','feeding','patting'],axis=1,inplace=True)\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    if len(X_train)%120 == 0:\n",
    "        \n",
    "        X_train = X_train.reshape(-1,seq,X_train.shape[1])\n",
    "        \n",
    "    else:\n",
    "        n = int(len(X_train)/120) + 1\n",
    "        m = n*120 - len(X_train)\n",
    "        z = np.zeros((m,2048))\n",
    "        X_train = np.vstack((X_train,z))\n",
    "        X_train = X_train.reshape(-1,seq,X_train.shape[1])\n",
    "        \n",
    "    X_val = np.array(X_val)\n",
    "    if len(X_val)%120 == 0:\n",
    "        \n",
    "        X_val = X_val.reshape(-1,seq,X_val.shape[1])\n",
    "        \n",
    "    else:\n",
    "        n = int(len(X_val)/120) + 1\n",
    "        m = n*120 - len(X_val)\n",
    "        z = np.zeros((m,2048))\n",
    "        X_val = np.vstack((X_val,z))\n",
    "        \n",
    "        X_val = X_val.reshape(-1,seq,X_val.shape[1]) \n",
    "        \n",
    "    \n",
    "   \n",
    "    train_labels = np.array(train_labels)\n",
    "    validation_labels = np.array(validation_labels)\n",
    "    model.fit(X_train,train_labels,batch_size=120,epochs=40,validation_split=0.2,callbacks=[es])\n",
    "    answer = model.predict(X_val)\n",
    "    pred.append(answer)\n",
    "    test.append(validation_labels)\n",
    "    #except:\n",
    "    #    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8.09753800e-02,  7.51085040e-01, -4.37508850e-01, ...,\n",
       "         -7.76825550e-01,  5.72837230e-01, -2.59454160e-01],\n",
       "        [ 2.51555610e-02, -6.60803800e-01, -1.11122030e-01, ...,\n",
       "          5.31326530e-01, -1.64661350e-01,  1.06762810e+00],\n",
       "        [-9.07105700e-01,  5.16185050e-01, -2.01945190e-01, ...,\n",
       "          3.18011460e-02, -5.39899950e-01,  9.36408160e-01],\n",
       "        ...,\n",
       "        [-3.47070540e-01,  7.55162060e-01, -1.28870210e+00, ...,\n",
       "         -8.85967730e-01,  7.35638740e-01,  4.63672400e-01],\n",
       "        [-3.59686670e-01,  7.40027400e-01, -3.70151500e-01, ...,\n",
       "         -1.99373400e-03,  1.95032720e-01, -5.12382030e-01],\n",
       "        [ 7.31410300e-01, -2.05360920e-01, -1.11234710e+00, ...,\n",
       "          3.00454380e-01,  6.67156600e-01, -1.92144980e-01]],\n",
       "\n",
       "       [[-4.68390350e-01,  1.49111380e-01, -8.57686640e-01, ...,\n",
       "          7.74056100e-01,  2.14054330e-01,  1.17967450e+00],\n",
       "        [ 6.91929740e-03,  7.66926350e-01, -8.85070600e-01, ...,\n",
       "          4.10388500e-01, -5.97938950e-01,  5.47029800e-01],\n",
       "        [-4.88235650e-01,  3.12821240e-01,  5.14656900e-02, ...,\n",
       "          4.36649440e-01,  9.51338900e-01,  1.53099330e-01],\n",
       "        ...,\n",
       "        [-7.19357250e-01,  2.71314010e-02, -2.77115100e-01, ...,\n",
       "          7.57449700e-01, -5.40012000e-01,  1.07746740e+00],\n",
       "        [ 4.12304100e-01, -8.07074360e-04, -4.86434460e-01, ...,\n",
       "          4.08974560e-01,  2.77684570e-01,  2.18041210e-01],\n",
       "        [-2.10346180e-01,  3.06960340e-01, -9.68442560e-01, ...,\n",
       "         -9.68585100e-01,  5.12838660e-01, -1.11600290e+00]],\n",
       "\n",
       "       [[-4.32635070e-01,  3.89631330e-01,  4.65814200e-01, ...,\n",
       "          2.67914300e-01, -8.45487500e-02,  2.06906350e-01],\n",
       "        [ 4.75423600e-01, -6.72510100e-01,  3.70112660e-01, ...,\n",
       "          7.01989770e-01, -2.11584760e-01,  7.39864100e-01],\n",
       "        [ 9.66609540e-01,  1.76142300e-01,  1.51149520e+00, ...,\n",
       "          2.80462620e-01, -1.92488240e-01, -5.52909400e-01],\n",
       "        ...,\n",
       "        [ 4.20608520e-01, -2.09486280e-01, -5.05667270e-01, ...,\n",
       "          1.20499540e+00,  2.32358490e-01,  9.28127170e-01],\n",
       "        [-8.96974100e-01, -6.11073400e-01, -7.40070160e-01, ...,\n",
       "          1.63196200e+00,  2.77455200e-01,  1.83765210e+00],\n",
       "        [ 8.91690250e-01, -1.83745200e-01, -4.87340180e-01, ...,\n",
       "          8.16358000e-01,  8.02917360e-01,  1.24228500e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.27731900e-01,  2.85590380e-01,  2.16692780e-01, ...,\n",
       "          1.66408080e-01,  1.13046100e+00,  1.09734830e+00],\n",
       "        [-1.36534800e-01,  4.52906460e-01,  5.43688900e-01, ...,\n",
       "          1.76506180e-01,  5.43471340e-01,  1.99417600e+00],\n",
       "        [ 2.35670080e-02,  1.48168410e-01, -2.88418380e-01, ...,\n",
       "          3.18429900e-01,  1.33951570e+00,  9.01326840e-01],\n",
       "        ...,\n",
       "        [ 1.57853880e+00, -2.53546300e-01,  1.20659040e+00, ...,\n",
       "          1.03656970e+00, -9.88341100e-01, -3.00874680e-01],\n",
       "        [ 7.18088270e-01, -3.22578250e-01, -5.78391400e-01, ...,\n",
       "          1.66568350e+00, -3.27218770e-01,  3.73694700e-01],\n",
       "        [-7.94269860e-01, -4.53702240e-01, -5.53586200e-01, ...,\n",
       "          1.37475380e+00,  8.88413300e-01, -1.39574680e-01]],\n",
       "\n",
       "       [[-1.64579400e+00,  1.39948130e-01, -6.21331040e-01, ...,\n",
       "         -4.18809100e-01,  1.48338390e+00,  1.57939660e+00],\n",
       "        [ 4.15690780e-01,  1.26761110e+00, -1.03465770e+00, ...,\n",
       "          2.38364400e+00,  3.79587470e-01,  1.74886440e+00],\n",
       "        [-2.52124800e-01, -1.29849680e+00,  9.85146900e-02, ...,\n",
       "          1.79793740e+00, -6.32646860e-01,  1.45604040e+00],\n",
       "        ...,\n",
       "        [ 6.23512860e-01, -6.00251200e-01,  2.59494930e-01, ...,\n",
       "          1.48717370e+00, -1.11648716e-01,  1.01015460e+00],\n",
       "        [ 1.70830790e+00, -2.92261570e-01, -1.00028630e+00, ...,\n",
       "          1.62175810e+00, -8.69706500e-02,  9.35062470e-01],\n",
       "        [ 2.32027100e-01, -9.50529050e-02, -4.33476360e-01, ...,\n",
       "          6.62076600e-01,  1.37780190e+00,  8.53380560e-01]],\n",
       "\n",
       "       [[-1.02976130e-01,  5.70348740e-01,  4.84556730e-01, ...,\n",
       "          2.89726600e-01, -1.00994320e+00,  4.72993400e-01],\n",
       "        [ 9.91451440e-01, -6.72733400e-01, -5.87408660e-01, ...,\n",
       "          1.39983260e+00, -9.70877900e-01,  1.62273620e+00],\n",
       "        [ 1.02457280e+00, -7.21222500e-01, -5.83929200e-01, ...,\n",
       "          1.38592980e+00, -3.60307100e-01,  1.00325500e+00],\n",
       "        ...,\n",
       "        [-2.81908960e-01,  1.57011890e+00, -1.16435460e+00, ...,\n",
       "          1.75766560e+00,  1.35302110e-01,  2.53282800e+00],\n",
       "        [-4.68653850e-02, -4.93308480e-01, -4.10413240e-01, ...,\n",
       "          1.76875960e+00, -4.18535840e-02,  1.71725670e+00],\n",
       "        [-3.46100060e-01,  4.12217560e-01,  9.61321000e-01, ...,\n",
       "          1.47887280e-02,  7.76761500e-01,  2.20065020e+00]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tp = []\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0.96479404 0.03280864 0.00239733]\n",
      "[0.00180136 0.97734195 0.02085672]\n",
      "[5.977019e-04 8.088140e-04 9.985935e-01]\n",
      "[5.3162180e-04 8.0230029e-04 9.9866605e-01]\n",
      "[5.1527814e-04 7.6086610e-04 9.9872380e-01]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[9.9810421e-01 1.5999384e-03 2.9581253e-04]\n",
      "[9.9832243e-01 1.2569895e-03 4.2055277e-04]\n",
      "[9.9802089e-01 1.5155555e-03 4.6353095e-04]\n",
      "[9.9825245e-01 1.3991847e-03 3.4835070e-04]\n",
      "[3.0961414e-04 2.9363259e-04 9.9939680e-01]\n",
      "[9.9826729e-01 1.5129090e-03 2.1983347e-04]\n",
      "[2.0542437e-04 2.3598109e-04 9.9955863e-01]\n",
      "[9.9845648e-01 1.2020039e-03 3.4152725e-04]\n",
      "[2.0347629e-04 2.3047191e-04 9.9956602e-01]\n",
      "[9.9825841e-01 1.4988742e-03 2.4273842e-04]\n",
      "[4.2144614e-04 9.9903971e-01 5.3872768e-04]\n",
      "[9.9836046e-01 1.4285129e-03 2.1105941e-04]\n",
      "[9.9828219e-01 1.4432103e-03 2.7469455e-04]\n",
      "[9.984654e-01 1.214835e-03 3.198247e-04]\n",
      "[9.9843723e-01 1.1280812e-03 4.3466111e-04]\n",
      "[9.9848276e-01 1.3105676e-03 2.0660943e-04]\n",
      "[9.985121e-01 1.256112e-03 2.317049e-04]\n",
      "[4.6065619e-04 9.9879229e-01 7.4706407e-04]\n",
      "[3.6028447e-04 3.8623440e-04 9.9925345e-01]\n",
      "[9.9808502e-01 1.3147115e-03 6.0022372e-04]\n",
      "[2.2167861e-04 2.4274645e-04 9.9953556e-01]\n",
      "[3.5275842e-04 3.2806309e-04 9.9931920e-01]\n",
      "[2.7224838e-04 2.7612798e-04 9.9945158e-01]\n",
      "[9.9844766e-01 1.3425609e-03 2.0971324e-04]\n",
      "[9.9821198e-01 1.4151722e-03 3.7279809e-04]\n",
      "[0.00596153 0.00186135 0.9921772 ]\n",
      "[0.99756026 0.00135156 0.00108827]\n",
      "[2.0271521e-04 2.3651421e-04 9.9956077e-01]\n",
      "[1.9952070e-04 2.3080641e-04 9.9956971e-01]\n",
      "[0.97129864 0.00933189 0.01936944]\n",
      "[9.9854279e-01 1.2477584e-03 2.0947236e-04]\n",
      "[9.9813771e-01 1.6933356e-03 1.6899026e-04]\n",
      "[9.9822277e-01 1.6033271e-03 1.7382187e-04]\n",
      "[0.02355443 0.00186082 0.9745848 ]\n",
      "[0.9888693  0.00795607 0.00317461]\n",
      "[9.9824357e-01 1.5453381e-03 2.1114037e-04]\n",
      "[0.00909119 0.0012469  0.98966193]\n",
      "[0.9915559  0.00657037 0.00187363]\n",
      "[1.9877215e-03 6.0452695e-04 9.9740773e-01]\n",
      "[0.97687846 0.01137564 0.01174586]\n",
      "[0.00782525 0.00135003 0.99082464]\n",
      "[0.99412906 0.00378253 0.00208836]\n",
      "[0.02795322 0.00211009 0.96993667]\n",
      "[2.0262873e-04 2.3692727e-04 9.9956042e-01]\n",
      "[0.9819995  0.01116709 0.00683342]\n",
      "[0.00847846 0.0011572  0.9903644 ]\n",
      "[0.9860874  0.00648205 0.00743049]\n",
      "[0.02697865 0.00227605 0.97074527]\n",
      "[0.98683304 0.00593965 0.00722735]\n",
      "[9.9844092e-01 1.3675321e-03 1.9150155e-04]\n",
      "[9.9693620e-01 2.7846801e-03 2.7920966e-04]\n",
      "[4.5290557e-04 9.9895465e-01 5.9239205e-04]\n",
      "[4.5654763e-04 9.9901128e-01 5.3211988e-04]\n",
      "[4.1219307e-04 9.9907351e-01 5.1421713e-04]\n",
      "[4.7100143e-04 9.9902093e-01 5.0810602e-04]\n",
      "[4.8028273e-04 9.9889588e-01 6.2389270e-04]\n",
      "[4.2932775e-04 9.9826318e-01 1.3074810e-03]\n",
      "[4.7105810e-04 9.9892104e-01 6.0785812e-04]\n",
      "[0.00190573 0.02776147 0.97033286]\n",
      "[0.85317445 0.12481063 0.02201493]\n",
      "[4.5738142e-04 9.9894959e-01 5.9301534e-04]\n",
      "[7.108928e-04 9.986222e-01 6.669106e-04]\n",
      "[6.0420425e-04 9.9889851e-01 4.9731822e-04]\n",
      "[3.7912087e-04 9.9907398e-01 5.4684933e-04]\n",
      "[7.2998187e-04 9.9876803e-01 5.0200307e-04]\n",
      "[4.0530285e-04 9.9884570e-01 7.4899243e-04]\n",
      "[4.8420805e-04 9.9896502e-01 5.5079791e-04]\n",
      "[6.9986004e-04 9.9878782e-01 5.1235658e-04]\n",
      "[4.5802037e-04 9.9895275e-01 5.8921403e-04]\n",
      "[5.238101e-04 9.989868e-01 4.894610e-04]\n",
      "[3.812780e-04 9.987770e-01 8.418124e-04]\n",
      "[4.8935425e-04 9.9895179e-01 5.5886171e-04]\n",
      "[6.1801728e-04 9.9876010e-01 6.2189397e-04]\n",
      "[3.5949957e-04 9.9911159e-01 5.2895403e-04]\n",
      "[3.874587e-04 9.991173e-01 4.952869e-04]\n",
      "[3.2389947e-04 9.9914944e-01 5.2670983e-04]\n",
      "[6.382340e-04 9.988489e-01 5.128508e-04]\n",
      "[4.5282507e-04 9.9901521e-01 5.3196016e-04]\n",
      "[5.835669e-04 9.988925e-01 5.239735e-04]\n",
      "[3.4369424e-04 9.9913293e-01 5.2336964e-04]\n",
      "[4.241720e-04 9.989856e-01 5.901850e-04]\n",
      "[1.076451e-03 9.982980e-01 6.256287e-04]\n",
      "[8.509863e-04 9.985812e-01 5.677848e-04]\n",
      "[3.2310002e-04 9.9912816e-01 5.4866850e-04]\n",
      "[4.6461343e-04 9.9900120e-01 5.3427002e-04]\n",
      "[5.607822e-04 9.987148e-01 7.244386e-04]\n",
      "[7.8790664e-04 9.9858153e-01 6.3053926e-04]\n",
      "[4.3404812e-04 9.9896550e-01 6.0039700e-04]\n",
      "[5.165672e-04 9.987919e-01 6.915536e-04]\n",
      "[4.019643e-04 9.990313e-01 5.667194e-04]\n",
      "[4.1030027e-04 9.9895287e-01 6.3685299e-04]\n",
      "[0.00352295 0.2642547  0.7322224 ]\n",
      "[0.84231406 0.13476591 0.02292005]\n",
      "[4.0309085e-04 9.9909008e-01 5.0686853e-04]\n",
      "[4.3726046e-04 9.9902201e-01 5.4070307e-04]\n",
      "[4.5410747e-04 9.9886274e-01 6.8317441e-04]\n",
      "[5.1490366e-03 9.9406874e-01 7.8221248e-04]\n",
      "[4.6378942e-04 9.9899703e-01 5.3927855e-04]\n",
      "[6.0116110e-04 9.9891996e-01 4.7896168e-04]\n",
      "[0.00159407 0.05614148 0.94226444]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[1.8249216e-04 2.3089135e-04 9.9958664e-01]\n",
      "[2.2707361e-04 2.4433853e-04 9.9952865e-01]\n",
      "[2.1027344e-04 2.3652572e-04 9.9955314e-01]\n",
      "[1.9625145e-04 2.3087583e-04 9.9957281e-01]\n",
      "[2.6580316e-04 2.6322607e-04 9.9947101e-01]\n",
      "[2.4770453e-04 2.5546883e-04 9.9949682e-01]\n",
      "[1.9236776e-04 2.2790641e-04 9.9957973e-01]\n",
      "[3.8336392e-04 3.1516238e-04 9.9930143e-01]\n",
      "[2.5065723e-04 2.5689177e-04 9.9949241e-01]\n",
      "[1.9940773e-04 2.3260205e-04 9.9956793e-01]\n",
      "[2.0711405e-04 2.3560302e-04 9.9955732e-01]\n",
      "[2.2659932e-04 2.4143720e-04 9.9953198e-01]\n",
      "[1.9677779e-04 2.2808019e-04 9.9957520e-01]\n",
      "[1.9592664e-04 2.3020813e-04 9.9957389e-01]\n",
      "[3.8192418e-04 3.6505723e-04 9.9925297e-01]\n",
      "[2.5794015e-04 2.6260898e-04 9.9947947e-01]\n",
      "[2.0008565e-04 2.3132049e-04 9.9956864e-01]\n",
      "[2.1046786e-04 2.4021426e-04 9.9954933e-01]\n",
      "[2.0667794e-04 2.3237648e-04 9.9956089e-01]\n",
      "[4.6513916e-04 3.5427217e-04 9.9918061e-01]\n",
      "[2.2822479e-04 2.4563554e-04 9.9952614e-01]\n",
      "[2.4394599e-04 2.5207835e-04 9.9950397e-01]\n",
      "[3.874630e-04 3.242335e-04 9.992884e-01]\n",
      "[2.2034455e-04 2.4261235e-04 9.9953711e-01]\n",
      "[2.0913121e-04 2.3824112e-04 9.9955255e-01]\n",
      "[2.7772129e-04 2.7158507e-04 9.9945074e-01]\n",
      "[2.0972859e-04 2.3958541e-04 9.9955064e-01]\n",
      "[2.0785550e-04 2.3379631e-04 9.9955839e-01]\n",
      "[2.0983258e-04 2.3767490e-04 9.9955243e-01]\n",
      "[2.0307930e-04 2.3072197e-04 9.9956614e-01]\n",
      "[2.2662040e-04 2.4285035e-04 9.9953055e-01]\n",
      "[1.8781175e-04 2.2396746e-04 9.9958819e-01]\n",
      "[2.5729087e-04 2.6874922e-04 9.9947399e-01]\n",
      "[2.0273593e-04 2.3056241e-04 9.9956673e-01]\n",
      "[2.4119407e-04 2.5687809e-04 9.9950194e-01]\n",
      "[2.8176032e-04 2.8660160e-04 9.9943167e-01]\n",
      "[2.9087492e-04 3.1359485e-04 9.9939549e-01]\n",
      "[2.253476e-04 2.439674e-04 9.995307e-01]\n",
      "[2.2755700e-04 2.4686172e-04 9.9952555e-01]\n",
      "[1.9719281e-04 2.3126844e-04 9.9957150e-01]\n",
      "[2.8442236e-04 2.7689443e-04 9.9943870e-01]\n",
      "[1.9594753e-04 2.3130224e-04 9.9957269e-01]\n",
      "[2.5545224e-04 2.6026560e-04 9.9948430e-01]\n",
      "[2.3122627e-04 2.4898778e-04 9.9951982e-01]\n",
      "[1.8762822e-04 2.2542628e-04 9.9958700e-01]\n",
      "[2.8150613e-04 2.7289687e-04 9.9944562e-01]\n",
      "[1.9623243e-04 2.3248352e-04 9.9957126e-01]\n",
      "[3.0743438e-04 2.8944813e-04 9.9940312e-01]\n",
      "[1.9073827e-04 2.2573228e-04 9.9958354e-01]\n",
      "[2.7840934e-04 2.6176518e-04 9.9945980e-01]\n",
      "[2.2866097e-04 2.4990144e-04 9.9952149e-01]\n",
      "[3.0858573e-04 2.8291318e-04 9.9940848e-01]\n",
      "[9.7585580e-04 6.1371148e-04 9.9841046e-01]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[9.9864477e-01 1.1627952e-03 1.9248168e-04]\n",
      "[9.9910307e-01 7.9828291e-04 9.8687749e-05]\n",
      "[1.7343294e-04 9.9925059e-01 5.7592161e-04]\n",
      "[9.9880850e-01 9.5237367e-04 2.3909562e-04]\n",
      "[1.4189068e-04 2.9004249e-04 9.9956805e-01]\n",
      "[9.9835396e-01 1.1903885e-03 4.5564456e-04]\n",
      "[9.9798018e-01 1.7960995e-03 2.2375544e-04]\n",
      "[1.9199263e-04 4.2158077e-04 9.9938643e-01]\n",
      "[2.1801697e-04 9.9940515e-01 3.7687094e-04]\n",
      "[9.9870539e-01 1.1227337e-03 1.7190853e-04]\n",
      "[1.3110600e-04 2.6534119e-04 9.9960357e-01]\n",
      "[9.9902678e-01 8.7423867e-04 9.8971155e-05]\n",
      "[9.9855667e-01 1.3225818e-03 1.2067461e-04]\n",
      "[1.4427974e-04 9.9941003e-01 4.4572912e-04]\n",
      "[2.1033332e-04 9.9943262e-01 3.5702199e-04]\n",
      "[9.9891102e-01 9.1800425e-04 1.7098048e-04]\n",
      "[9.9874687e-01 1.1056979e-03 1.4742906e-04]\n",
      "[2.010732e-04 9.990289e-01 7.700562e-04]\n",
      "[9.99022722e-01 8.72579170e-04 1.04683095e-04]\n",
      "[9.9561834e-01 4.1154986e-03 2.6618829e-04]\n",
      "[9.9879241e-01 1.1025021e-03 1.0495862e-04]\n",
      "[9.9860269e-01 1.3060652e-03 9.1306043e-05]\n",
      "[1.4656216e-04 9.9946922e-01 3.8428104e-04]\n",
      "[3.7365462e-04 7.3595735e-04 9.9889046e-01]\n",
      "[1.6717965e-04 9.9939942e-01 4.3345342e-04]\n",
      "[1.6607338e-04 9.9947220e-01 3.6170011e-04]\n",
      "[1.9058234e-04 9.9933428e-01 4.7511826e-04]\n",
      "[2.5454405e-04 9.9915028e-01 5.9518404e-04]\n",
      "[2.7924695e-04 9.9904329e-01 6.7744107e-04]\n",
      "[1.6530225e-04 9.9937278e-01 4.6189837e-04]\n",
      "[1.7925948e-04 9.9938071e-01 4.4000559e-04]\n",
      "[2.7149054e-04 9.9799061e-01 1.7379256e-03]\n",
      "[1.3738402e-04 2.8577526e-04 9.9957687e-01]\n",
      "[1.4669758e-04 3.1704089e-04 9.9953628e-01]\n",
      "[3.0020819e-04 9.5115532e-04 9.9874866e-01]\n",
      "[2.3887584e-04 9.9894923e-01 8.1189559e-04]\n",
      "[1.7728348e-04 9.9943525e-01 3.8746468e-04]\n",
      "[1.5924122e-04 9.9940515e-01 4.3552252e-04]\n",
      "[2.3850694e-04 9.9904007e-01 7.2132528e-04]\n",
      "[1.5420494e-04 9.9940825e-01 4.3760557e-04]\n",
      "[1.4622477e-04 9.9937421e-01 4.7960671e-04]\n",
      "[1.5561641e-04 9.9940455e-01 4.3980233e-04]\n",
      "[1.2545037e-04 2.1746008e-04 9.9965703e-01]\n",
      "[1.2545587e-04 2.1746030e-04 9.9965703e-01]\n",
      "[1.6053482e-04 3.5063224e-04 9.9948883e-01]\n",
      "[1.4865369e-04 3.1454954e-04 9.9953687e-01]\n",
      "[1.6072436e-04 3.1016971e-04 9.9952912e-01]\n",
      "[1.5249051e-04 3.4503109e-04 9.9950242e-01]\n",
      "[1.5332417e-04 3.2048952e-04 9.9952614e-01]\n",
      "[2.3114133e-04 6.4474071e-04 9.9912411e-01]\n",
      "[2.3294550e-04 6.2980124e-04 9.9913728e-01]\n",
      "[3.271461e-04 7.815464e-04 9.988913e-01]\n",
      "[0.06462055 0.13769898 0.79768044]\n",
      "[1.6736452e-04 4.2671416e-04 9.9940598e-01]\n",
      "[1.5800097e-04 3.2601974e-04 9.9951601e-01]\n",
      "[1.6429383e-04 3.2891356e-04 9.9950683e-01]\n",
      "[1.3266146e-04 2.6066587e-04 9.9960667e-01]\n",
      "[4.8176749e-04 7.5899751e-04 9.9875927e-01]\n",
      "[4.1326610e-04 1.0746042e-03 9.9851209e-01]\n",
      "[9.748343e-04 9.590836e-04 9.980661e-01]\n",
      "[2.0552201e-04 3.4710314e-04 9.9944741e-01]\n",
      "[1.5166348e-04 3.3486419e-04 9.9951351e-01]\n",
      "[2.0717613e-04 3.7159285e-04 9.9942124e-01]\n",
      "[2.0135839e-04 4.6945902e-04 9.9932921e-01]\n",
      "[1.7052601e-04 3.4846307e-04 9.9948102e-01]\n",
      "[1.4950421e-04 2.8386744e-04 9.9956661e-01]\n",
      "[1.3699471e-04 2.7997713e-04 9.9958307e-01]\n",
      "[1.8003074e-04 3.1091899e-04 9.9950910e-01]\n",
      "[1.3698702e-04 2.7134633e-04 9.9959165e-01]\n",
      "[1.4722411e-04 3.1193157e-04 9.9954093e-01]\n",
      "[1.2545576e-04 2.1746050e-04 9.9965703e-01]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[8.5755077e-05 1.5514724e-04 9.9975914e-01]\n",
      "[8.8628411e-05 1.4083166e-04 9.9977058e-01]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[9.9949718e-01 3.8184103e-04 1.2087873e-04]\n",
      "[8.0799095e-05 9.9976939e-01 1.4978202e-04]\n",
      "[8.3540843e-05 9.9972624e-01 1.9016862e-04]\n",
      "[7.3874638e-05 9.9979466e-01 1.3145537e-04]\n",
      "[6.7969311e-05 1.0612228e-04 9.9982589e-01]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[9.9945933e-01 4.7913252e-04 6.1570667e-05]\n",
      "[5.2742405e-05 7.1967632e-05 9.9987531e-01]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[7.488987e-05 9.998367e-01 8.845888e-05]\n",
      "[5.4845608e-05 9.9985409e-01 9.1048765e-05]\n",
      "[6.0848957e-05 9.9984097e-01 9.8141994e-05]\n",
      "[5.9961494e-05 9.9985170e-01 8.8343098e-05]\n",
      "[5.9317565e-05 9.9985111e-01 8.9533802e-05]\n",
      "[6.5967404e-05 1.0120351e-04 9.9983275e-01]\n",
      "[4.5362736e-05 6.1834267e-05 9.9989283e-01]\n",
      "[4.293225e-05 6.576949e-05 9.998913e-01]\n",
      "[5.046598e-05 7.188542e-05 9.998777e-01]\n",
      "[4.3719192e-05 6.4788757e-05 9.9989152e-01]\n",
      "[4.9324473e-05 6.7695342e-05 9.9988294e-01]\n",
      "[4.276643e-05 6.395460e-05 9.998933e-01]\n",
      "[4.3912689e-05 6.6946908e-05 9.9988914e-01]\n",
      "[4.3684649e-05 6.5649765e-05 9.9989069e-01]\n",
      "[8.1346050e-05 1.1288450e-04 9.9980575e-01]\n",
      "[4.5349501e-05 6.1839935e-05 9.9989283e-01]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[5.6552664e-05 9.9987030e-01 7.3133648e-05]\n",
      "[9.9975771e-01 2.0654601e-04 3.5734862e-05]\n",
      "[5.1081017e-05 9.9985659e-01 9.2257025e-05]\n",
      "[6.451113e-05 9.998572e-01 7.831445e-05]\n",
      "[5.5600412e-05 9.9986982e-01 7.4602081e-05]\n",
      "[4.4997858e-05 6.7301065e-05 9.9988770e-01]\n",
      "[4.4192609e-05 6.4770073e-05 9.9989104e-01]\n",
      "[3.9691131e-05 5.8053636e-05 9.9990225e-01]\n",
      "[4.1884952e-05 6.1460887e-05 9.9989665e-01]\n",
      "[3.8997470e-05 5.9197988e-05 9.9990177e-01]\n",
      "[3.9748524e-05 5.9824990e-05 9.9990046e-01]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[9.9978298e-01 1.7054810e-04 4.6475292e-05]\n",
      "[4.5572142e-05 9.9988556e-01 6.8851994e-05]\n",
      "[4.5926641e-05 6.5501779e-05 9.9988854e-01]\n",
      "[5.8170237e-05 1.0348183e-04 9.9983835e-01]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[4.3129861e-05 9.9986875e-01 8.8024528e-05]\n",
      "[3.4916433e-05 5.6335044e-05 9.9990869e-01]\n",
      "[4.5816789e-05 9.9989510e-01 5.9117876e-05]\n",
      "[3.6937017e-05 9.9990332e-01 5.9748585e-05]\n",
      "[3.8938924e-05 5.3967797e-05 9.9990714e-01]\n",
      "[3.1720603e-05 4.6257865e-05 9.9992204e-01]\n",
      "[3.3621287e-05 4.7429261e-05 9.9991894e-01]\n",
      "[3.2728327e-05 4.3694796e-05 9.9992359e-01]\n",
      "[3.1830925e-05 4.5608438e-05 9.9992251e-01]\n",
      "[3.3003205e-05 4.9001796e-05 9.9991798e-01]\n",
      "[3.2902699e-05 4.7361274e-05 9.9991977e-01]\n",
      "[3.6374600e-05 5.2496842e-05 9.9991107e-01]\n",
      "[3.2727075e-05 4.3695043e-05 9.9992359e-01]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[2.9806541e-05 3.9503786e-05 9.9993074e-01]\n",
      "[9.99862313e-01 1.16514646e-04 2.10662984e-05]\n",
      "[9.9987042e-01 1.0994851e-04 1.9615520e-05]\n",
      "[2.9962832e-05 3.9443476e-05 9.9993062e-01]\n",
      "[4.2467109e-05 5.7296846e-05 9.9990022e-01]\n",
      "[9.9980479e-01 1.6479717e-04 3.0366549e-05]\n",
      "[9.9975330e-01 2.2527078e-04 2.1439953e-05]\n",
      "[9.9963987e-01 3.0830034e-04 5.1828883e-05]\n",
      "[9.9981195e-01 1.6713115e-04 2.0948542e-05]\n",
      "[2.9576799e-05 3.9146064e-05 9.9993122e-01]\n",
      "[3.574730e-05 4.938931e-05 9.999149e-01]\n",
      "[2.9171528e-05 4.2763349e-05 9.9992812e-01]\n",
      "[3.7144026e-05 5.5436034e-05 9.9990737e-01]\n",
      "[2.8968128e-05 4.2835683e-05 9.9992824e-01]\n",
      "[3.6623980e-05 5.3775784e-05 9.9990964e-01]\n",
      "[2.9677092e-05 3.9464179e-05 9.9993086e-01]\n",
      "[2.9177789e-05 4.1805022e-05 9.9992907e-01]\n",
      "[2.8960318e-05 4.2616884e-05 9.9992847e-01]\n",
      "[3.305674e-05 4.643131e-05 9.999205e-01]\n",
      "[3.1208852e-05 4.2664560e-05 9.9992609e-01]\n",
      "[3.4200355e-05 4.8664398e-05 9.9991715e-01]\n",
      "[3.754281e-05 6.117866e-05 9.999013e-01]\n",
      "[2.9426310e-05 3.9222901e-05 9.9993134e-01]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[4.7666075e-05 9.9990416e-01 4.8140275e-05]\n",
      "[3.9178547e-05 9.9989665e-01 6.4123065e-05]\n",
      "[3.6843987e-05 9.9991167e-01 5.1484290e-05]\n",
      "[4.7701054e-05 9.9989295e-01 5.9396865e-05]\n",
      "[3.3501110e-05 9.9991250e-01 5.4049415e-05]\n",
      "[3.1139134e-05 9.9992216e-01 4.6774581e-05]\n",
      "[2.5023553e-05 3.6383095e-05 9.9993861e-01]\n",
      "[2.5712752e-05 3.7361558e-05 9.9993694e-01]\n",
      "[2.7170254e-05 3.5898134e-05 9.9993694e-01]\n",
      "[2.7312926e-05 3.5961788e-05 9.9993670e-01]\n",
      "[2.9118590e-05 3.9544218e-05 9.9993134e-01]\n",
      "[2.5513904e-05 3.6288478e-05 9.9993825e-01]\n",
      "[2.4103867e-05 3.4665965e-05 9.9994123e-01]\n",
      "[3.5373392e-05 4.8261849e-05 9.9991632e-01]\n",
      "[3.0419238e-05 4.2896936e-05 9.9992669e-01]\n",
      "[3.4957426e-05 4.7418300e-05 9.9991763e-01]\n",
      "[2.6351578e-05 3.8413822e-05 9.9993527e-01]\n",
      "[2.7559594e-05 4.0108916e-05 9.9993229e-01]\n",
      "[2.5972509e-05 3.7162416e-05 9.9993682e-01]\n",
      "[2.5791347e-05 3.6998095e-05 9.9993718e-01]\n",
      "[3.3213280e-05 5.1324478e-05 9.9991548e-01]\n",
      "[2.9434053e-05 4.2264590e-05 9.9992836e-01]\n",
      "[2.7605432e-05 3.9504586e-05 9.9993289e-01]\n",
      "[4.3237145e-05 5.1778843e-05 9.9990499e-01]\n",
      "[2.9186236e-05 4.0250394e-05 9.9993050e-01]\n",
      "[2.9068280e-05 4.7514681e-05 9.9992347e-01]\n",
      "[2.5284819e-05 3.5987148e-05 9.9993873e-01]\n",
      "[2.4894627e-05 3.6008823e-05 9.9993908e-01]\n",
      "[2.6573185e-05 3.7876071e-05 9.9993551e-01]\n",
      "[2.5410647e-05 3.6527668e-05 9.9993801e-01]\n",
      "[3.5126977e-05 5.7273613e-05 9.9990761e-01]\n",
      "[3.1232925e-05 4.4102733e-05 9.9992466e-01]\n",
      "[2.5267704e-05 3.6945265e-05 9.9993777e-01]\n",
      "[8.0220787e-05 8.0853366e-05 9.9983895e-01]\n",
      "[2.5548752e-05 3.6874997e-05 9.9993753e-01]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[9.9992108e-01 6.6086955e-05 1.2825490e-05]\n",
      "[9.99913931e-01 7.37864175e-05 1.22505635e-05]\n",
      "[9.9992263e-01 6.3704407e-05 1.3691095e-05]\n",
      "[9.9988711e-01 9.7243901e-05 1.5568554e-05]\n",
      "[9.9990737e-01 7.7996978e-05 1.4684857e-05]\n",
      "[9.9990439e-01 8.0231097e-05 1.5370513e-05]\n",
      "[9.9989951e-01 8.6270316e-05 1.4139293e-05]\n",
      "[9.99867439e-01 1.16708856e-04 1.57954146e-05]\n",
      "[1.9435020e-05 2.4861207e-05 9.9995565e-01]\n",
      "[0.9864108  0.00617005 0.00741915]\n",
      "[0.04815727 0.871203   0.08063975]\n",
      "[0.02502455 0.97176886 0.00320656]\n",
      "[1.9435020e-05 2.4861207e-05 9.9995565e-01]\n",
      "[1.1749483e-02 9.8786467e-01 3.8578085e-04]\n",
      "[0.89963114 0.09886665 0.00150214]\n",
      "[2.6548170e-02 9.7298467e-01 4.6710297e-04]\n",
      "[0.00253956 0.9912314  0.00622902]\n",
      "[0.714383   0.2810218  0.00459523]\n",
      "[1.5202462e-04 4.7669129e-04 9.9937135e-01]\n",
      "[8.4241760e-01 1.5704735e-01 5.3502800e-04]\n",
      "[1.9435020e-05 2.4861207e-05 9.9995565e-01]\n",
      "[1.0779321e-03 9.9865568e-01 2.6628224e-04]\n",
      "[0.01239072 0.98441446 0.00319483]\n",
      "[0.00497004 0.9921034  0.00292651]\n",
      "[2.0550699e-05 3.0655570e-05 9.9994874e-01]\n",
      "[1.8220491e-05 2.5502442e-05 9.9995625e-01]\n",
      "[1.9161513e-05 2.4642732e-05 9.9995625e-01]\n",
      "[2.2868026e-05 2.9474199e-05 9.9994767e-01]\n",
      "[1.9452380e-05 2.6641119e-05 9.9995387e-01]\n",
      "[1.941220e-05 2.485450e-05 9.999558e-01]\n",
      "[2.0063673e-05 2.6385882e-05 9.9995351e-01]\n",
      "[1.9434687e-05 2.4861160e-05 9.9995565e-01]\n",
      "[2.0238764e-05 2.7382888e-05 9.9995244e-01]\n",
      "[1.8744506e-05 2.5886960e-05 9.9995542e-01]\n",
      "[2.7593971e-05 3.5235851e-05 9.9993718e-01]\n",
      "[1.8598204e-05 2.6200745e-05 9.9995518e-01]\n",
      "[1.977921e-05 2.872002e-05 9.999515e-01]\n",
      "[1.8891333e-05 2.5994790e-05 9.9995506e-01]\n",
      "[1.8497150e-05 2.5407206e-05 9.9995613e-01]\n",
      "[1.8272161e-05 2.5044779e-05 9.9995673e-01]\n",
      "[1.8029639e-05 2.6007758e-05 9.9995601e-01]\n",
      "[2.0838372e-05 2.9056802e-05 9.9995005e-01]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test)):\n",
    "    p = pred[i]\n",
    "    t = test[i]\n",
    "    \n",
    "    for j in t:\n",
    "        print(j)\n",
    "        y_tp.append(np.argmax(j))\n",
    "    for k in p:\n",
    "        print(k)\n",
    "        y_pred.append(np.argmax(k))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        63\n",
      "           1       1.00      0.89      0.94       108\n",
      "           2       0.98      1.00      0.99       221\n",
      "\n",
      "    accuracy                           0.97       392\n",
      "   macro avg       0.96      0.96      0.96       392\n",
      "weighted avg       0.97      0.97      0.97       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_tp,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63   0   0]\n",
      " [  7  96   5]\n",
      " [  0   0 221]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_tp,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
